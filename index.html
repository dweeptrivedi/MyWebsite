<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Dweep Trivedi</title>
  
  <meta name="author" content="Dweep Trivedi">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon1.png">
</head>
  
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/dweep_image1.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/dweep_image1.jpeg" class="hoverZoomLink"></a>
            </td>
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Dweep Trivedi</name>
              </p>
              <p>I am a visiting researcher at <a href="https://www.clvrai.com/"> Cognitive Learning for Vision and Robotics Lab (CLVR) </a> at <a href= "https://www.cs.usc.edu/">University of Southern California</a>, advised by <a href="https://viterbi-web.usc.edu/~limjj/"> Prof. Joseph Lim </a>.
              </p>
              <p>
                Previously, I finished my Masterâ€™s in Computer Science at <a href="https://www.cs.usc.edu/">University of Southern California</a>, where I worked under the guidance of <a href= "https://sites.google.com/view/skim-home/home" >Dr. Seon Ho Kim</a> and <a href= "https://viterbi-web.usc.edu/~liu32/">Prof. Yan Liu</a>. Earlier, I spent 2 years working as Software Development Engineer in <a href="https://www.juniper.net/us/en.html" >Juniper Networks, Bangalore </a>. I did my undergrad in Information and Communication Technology at <a href="https://www.daiict.ac.in/"> DA-IICT, Gandhinagar</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:dweeptrivedi1994@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/DweepTrivedi_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Jf9i6GQAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                
                <a href="https://github.com/dweeptrivedi">Github</a>
                <a href="https://twitter.com/dweeptrivedi">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/dweep-trivedi">Linkedin</a> &nbsp/&nbsp
              </p>
            </td>
           
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in Deep Learning, Interpretable Reinforcement Learning, Continual Learning and Robot Learning. 
              </p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/paper1_image.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/dweep_image.jpg" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://clvrai.github.io/leaps/">
                <papertitle>Learning to Synthesize Programs as Interpretable and Generalizable Policies</papertitle>
              </a>
              <br>
              <strong>Dweep Trivedi</strong>,
              <a href="https://jesbu1.github.io/">Jesse Zhang</a>,
              <a href="https://shaohua0116.github.io/">Shao-Hua Sun</a>,
              <a href="https://viterbi-web.usc.edu/~limjj/">Joseph J Lim</a>,
              
              <br>
        <em>Thirty-Fifth Conference on Neural Information Processing Systems</em>, 2021
              <br>
              <a href="https://papers.nips.cc/paper/2021/file/d37124c4c79f357cb02c655671a432fa-Paper.pdf">PDF</a>
        /
              <a href="https://shaohua0116.github.io/bibtex/leaps.txt">BibTex</a>
       
              <p></p>
              <p>We present a framework that learns to synthesize a program, detailing the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task.</p>
            </td>
          </tr>
          
          <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/paper1_image.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/dweep_image.jpg" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>
                <papertitle>Yet another deep learning approach for road damage detection using ensemble learning
              </strong>
              <br>
              Vinuta Hegde, 
              <strong>Dweep Trivedi</strong>,
              <a href="http://www-scf.usc.edu/~alfarrar/">Abdullah Alfarrarjeh</a>,
              Aditi Deepak,
              <a href="https://sites.google.com/view/skim-home/home">Seon Ho Kim</a>,
               Cyrus
              
              <br>
        <em>IEEE International Conference on Big Data (Big Data)</em>, 2020
              <br>
              <a href="https://infolab.usc.edu/DocsDemos/BigDataRDD2020.pdf">PDF</a>
        /
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:5qvy8tD3MEUJ:scholar.google.com/&output=citation&scisdr=CgXmK2d-EMDjhTfn3Ws:AAGBfm0AAAAAYbDhxWv_b-V2-WnxjTL0ntpFPxVpa-Nr&scisig=AAGBfm0AAAAAYbDhxYmvxulS7zaDmj74DPo9mqUtUAEy&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
       
              <p></p>
              <p>We present a framework that learns to synthesize a program, detailing the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task.</p>
            </td>
          </tr>
          
          <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/paper1_image.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/dweep_image.jpg" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>
                <papertitle>Multi-agent trajectory prediction with fuzzy query attention
              </strong>
              <br>
              Nitin Kamra, 
              Hao Zhu,
              <strong>Dweep Trivedi</strong>,
              Ming Zhang,
              Yan Liu
              
              <br>
        <em>NeurIPS</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2010.15891.pdf">PDF</a>
        /
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:l6KJwi0ecywJ:scholar.google.com/&output=citation&scisdr=CgXmK2d-EMDjhTfnOv8:AAGBfm0AAAAAYbDhIv-G-U77IJsclkI8rJ1GZ-DVwD8M&scisig=AAGBfm0AAAAAYbDhIpTFxai8BpN8585JgnK0O5b-xArf&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
       
              <p></p>
              <p>We present a framework that learns to synthesize a program, detailing the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task.</p>
            </td>
          </tr>
          
          
          <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/paper1_image.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/dweep_image.jpg" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>
                <papertitle>Recognizing material of a covered object: A case study with graffiti
              </strong>
              <br>
              
              <a href="http://www-scf.usc.edu/~alfarrar/">Abdullah Alfarrarjeh</a>,
              <strong>Dweep Trivedi</strong>,
                <a href="https://sites.google.com/view/skim-home/home">Seon Ho Kim</a>,
              Hyunjun Park, Chao Huang, Cyrus Shahabi
              
              <br>
        <em>IEEE International Conference on Image Processing (ICIP)</em>, 2019
              <br>
              <a href="http://mediaq.usc.edu:8080/TVDP/papers/IEEE_ICIP_graffiti.pdf">PDF</a>
        /
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:QKX1K4VLOoYJ:scholar.google.com/&output=citation&scisdr=CgXmK2d-EMDjhTfYHSQ:AAGBfm0AAAAAYbDeBSQgHDgafaPXkXW4ozuN3Zl7NKmp&scisig=AAGBfm0AAAAAYbDeBYEtJ6ZOjmgmASLwtPqifHwD4Vqa&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
       
              <p></p>
              <p>We present a framework that learns to synthesize a program, detailing the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task.</p>
            </td>
          </tr>
          
          <tr onmouseout="clipnerf_stop()" onmouseover="clipnerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <a href="images/paper1_image.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/dweep_image.jpg" class="hoverZoomLink"></a>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <strong>
                <papertitle>A deep learning approach for road damage detection from smartphone images
              </strong>
              <br>
              
              <a href="http://www-scf.usc.edu/~alfarrar/">Abdullah Alfarrarjeh</a>,
              <strong>Dweep Trivedi</strong>,
                <a href="https://sites.google.com/view/skim-home/home">Seon Ho Kim</a>,
              Cyrus Shahabi
              
              <br>
        <em>IEEE International Conference on Big Data (Big Data)</em>, 2018
              <br>
              <a href="https://infolab.usc.edu/DocsDemos/IEEE_BigData_RoadDamageDetection.pdf">PDF</a>
        /
              <a href="https://scholar.googleusercontent.com/scholar.bib?q=info:ixejBOKojc8J:scholar.google.com/&output=citation&scisdr=CgXmK2d-EMDjhTfZdiA:AAGBfm0AAAAAYbDfbiD_Wg1qor_1SaVIwdjEbj1HkTY_&scisig=AAGBfm0AAAAAYbDfbhEIDPwXaU2bZHslBnbTItXdJbO0&scisf=4&ct=citation&cd=-1&hl=en">BibTex</a>
       
              <p></p>
              <p>We present a framework that learns to synthesize a program, detailing the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task.</p>
            </td>
          </tr>
        
        </body>
        

</html>
